## Loading the data
data <- read.csv("household_power_consumption.txt", header = T, sep = ";",na.string = "?", nrows = 2075259, check.names = F, stringsAsFactors = F,comment.char = "", quote = "\"'")
## Subsetting the data
subSetData <- data[data$Date %in% c("1/2/2007","2/2/2007") ,]
## Converting
Str_Datetime <- strptime(paste(subSetData$Date, subSetData$Time, sep=" "), "%d/%m/%Y %H:%M:%S")
num_submetering1 <- as.numeric(subSetData$Sub_metering_1)
num_submetering2 <- as.numeric(subSetData$Sub_metering_2)
num_submetering3 <- as.numeric(subSetData$Sub_metering_3)
## Plot3
png("plot3.png", width = 480, height = 480)
plot(Str_Datetime, num_submetering1, type = "l", xlab = "", ylab = "Energy Sub metering")
lines(Str_Datetime, num_submetering2, col = "Red")
lines(Str_Datetime, num_submetering3, col = "Blue")
legend("topright", c("Sub_metering_1", "Sub_metering_2", "Sub_metering_3"), lty = 1, lwd = 2.5, col = c("black", "red", "blue"))
dev.off()
install.packages("KernSmooth")
library(KernSmooth)
svmrfeFeatureRanking = function(x,y){
setwd("/Users/Shuan/Documents/R_Programming")
# load data
data <- read.csv("NormPKCLALL.csv", header = T)
dat.x <- data[,1:24]
dat.y <- data[,25]
# scale data
#scaled.x <- scale(dat.x)
# check that we get mean of 0 and sd of 1
colMeans(dat.x)  # faster version of apply(scaled.dat, 2, mean)
apply(dat.x, 2, sd)
# svm-rfe
library("e1071")
svmrfeFeatureRanking = function(x,y){
n = ncol(x)
survivingFeaturesIndexes = seq(1:n)
featureRankedList = vector(length=n)
rankedFeatureIndex = n
while(length(survivingFeaturesIndexes)>0){
#train the support vector machine
svmModel = svm(x[, survivingFeaturesIndexes], y, cost = 10, cachesize=500,
scale=F, type="C-classification", kernel="linear" )
#compute the weight vector
w = t(svmModel$coefs)%*%svmModel$SV
#compute ranking criteria
rankingCriteria = w * w
#rank the features
ranking = sort(rankingCriteria, index.return = TRUE)$ix
#update feature ranked list
featureRankedList[rankedFeatureIndex] = survivingFeaturesIndexes[ranking[1]]
rankedFeatureIndex = rankedFeatureIndex - 1
#eliminate the feature with smallest ranking criterion
(survivingFeaturesIndexes = survivingFeaturesIndexes[-ranking[1]])
}
return (featureRankedList)
}
svmrfeFeatureRanking(dat.x,dat.y)
# load data
data <- read.csv("NormPKCLALL.csv", header = T)
dat.x <- data[,1:24]
dat.y <- data[,25]
# scale data
#scaled.x <- scale(dat.x)
# check that we get mean of 0 and sd of 1
#colMeans(dat.x)  # faster version of apply(scaled.dat, 2, mean)
#apply(dat.x, 2, sd)
# svm-rfe
library("e1071")
svmrfeFeatureRanking = function(x,y){
n = ncol(x)
survivingFeaturesIndexes = seq(1:n)
featureRankedList = vector(length=n)
rankedFeatureIndex = n
while(length(survivingFeaturesIndexes)>0){
#train the support vector machine
svmModel = svm(x[, survivingFeaturesIndexes], y, cost = 10, cachesize=500,
scale=F, type="C-classification", kernel="linear" )
#compute the weight vector
w = t(svmModel$coefs)%*%svmModel$SV
#compute ranking criteria
rankingCriteria = w * w
#rank the features
ranking = sort(rankingCriteria, index.return = TRUE)$ix
#update feature ranked list
featureRankedList[rankedFeatureIndex] = survivingFeaturesIndexes[ranking[1]]
rankedFeatureIndex = rankedFeatureIndex - 1
#eliminate the feature with smallest ranking criterion
(survivingFeaturesIndexes = survivingFeaturesIndexes[-ranking[1]])
}
return (featureRankedList)
}
svmrfeFeatureRanking(dat.x,dat.y)
setwd("/Users/Shuan/Documents/R_Programming")
# load data
data <- read.csv("NormPKCLALL.csv", header = T)
dat.x <- data[,1:24]
dat.y <- data[,25]
# scale data
#scaled.x <- scale(dat.x)
# check that we get mean of 0 and sd of 1
#colMeans(dat.x)  # faster version of apply(scaled.dat, 2, mean)
#apply(dat.x, 2, sd)
# svm-rfe
library("e1071")
svmrfeFeatureRanking = function(x,y){
n = ncol(x)
survivingFeaturesIndexes = seq(1:n)
featureRankedList = vector(length=n)
rankedFeatureIndex = n
while(length(survivingFeaturesIndexes)>0){
#train the support vector machine
svmModel = svm(x[, survivingFeaturesIndexes], y, cost = 10, cachesize=500,
scale=F, type="C-classification", kernel="linear" )
#compute the weight vector
w = t(svmModel$coefs)%*%svmModel$SV
#compute ranking criteria
rankingCriteria = w * w
#rank the features
ranking = sort(rankingCriteria, index.return = TRUE)$ix
#update feature ranked list
featureRankedList[rankedFeatureIndex] = survivingFeaturesIndexes[ranking[1]]
rankedFeatureIndex = rankedFeatureIndex - 1
#eliminate the feature with smallest ranking criterion
(survivingFeaturesIndexes = survivingFeaturesIndexes[-ranking[1]])
}
return (featureRankedList)
}
svmrfeFeatureRanking(dat.x,dat.y)
source('~/Documents/R_Programming/svm_rfe.R')
Q
install.packages(pathClass)
install.packages('pathClass')
install.packages('golubEsets')
installing *source* package ‘golubEsets’
help('golubEsets')
setRepositories()
install.packages("RMySQL")
ucscDb <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.usuc.edu")
result <- dbGetQuery(ucscDb,"show databases;");dbDiconnect(ucscDb);
library("RMySQL")
ucscDb <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.usuc.edu")
result <- dbGetQuery(ucscDb,"show databases;");dbDiconnect(ucscDb);
library("RMySQL")
ucscDb <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.usuc.edu")
result <- dbGetQuery(ucscDb,"show databases;");dbDiconnect(ucscDb);
library("RMySQL")
ucscDb <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.usuc.edu")
result <- dbGetQuery(ucscDb,"show databases;");dbDiconnect(ucscDb);
library(datasets)
data(iris)
?iris
install.packages("datasets")
install.packages("datasets")
install.packages("datasets")
install.packages("datasets")
iris
View(iris)
cm <- colMeans(iris$Sepal.Length)
cm <- mean(iris$Sepal.Length)
View(iris)
View(iris)
View(iris)
split(iris,iris$Species, drop=False)
split(iris,iris$Species)
split_dat <- split(iris,iris$Species)
load(iris)
load(virginica)
tapply(iris,iris$species,mean)
sapply(split_dat$virginica,mean)
split_dat[3]
cm <- mean(split_dat[3]$Sepal.Length)
sapply(split_, mean)
sapply(split_dat, mean)
library(datasets)
data(iris)
split_dat <- split(iris,iris$Species)
sapply(split_dat, mean)
virginica <- split_dat[3]
mean(virginica$Sepal.Lenth)
mean(virginica$Sepal.Lenth, na.rm = T)
mean(virginica[Sepal.Lenth], na.rm = T)
mean(virginica[1], na.rm = T)
library(datasets)
data(iris)
?iris
View(iris)
virginica <- subset(iris, Species == "virginica")
virginica
summary(virginica)
setwd("~/Desktop/ProgrammingAssignment2")
makeCacheMatrix <- function(x = matrix()) {
inv <- NULL
set <- function(y) {
x <<- y
inv <<- NULL
}
get <- function() x
setInverse <- function(inverse) inv <<- inverse
getInverse <- function() inv
list(set = set,
get = get,
setInverse = setInverse,
getInverse = getInverse)
}
## CacheSolve: This function computes the inverse of the special "matrix"
## returned by makeCacheMatrix above. If the inverse has already been calculated
## (and the matrix has not changed), then the cachesolve should retrieve the
## inverse from the cache.
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
inv <- x$getInverse()
if (!is.null(inv)) {
message("getting cached data")
return(inv)
}
mat <- x$get()
inv <- solve(mat, ...)
x$setInverse(inv)
inv
}
a <- c(2,2;3,5;6,8)
a <- c(rep(3,6),3)
a <- (c(rep(6,10),2,5)
a <- (c(rep(6,10),2,5))
a <- (rep(6,10),2,5)
a <- (c(3,4,5,6,7,8),2,3)
a <- matrix(c(3,4,5,6,7,8),2,3)
cacheSolve(a)
makeCacheMatrix(a)
cacheSolve(a)
